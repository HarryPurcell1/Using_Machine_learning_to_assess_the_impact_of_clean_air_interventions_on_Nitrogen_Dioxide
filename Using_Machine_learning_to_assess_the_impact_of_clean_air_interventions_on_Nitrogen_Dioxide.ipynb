{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0ugMKdI_3So"
      },
      "source": [
        "# **Using Machine learning to assess the impact of 'clean air' interventions on Nitrogen Dioxide (NO₂)**\n",
        "\n",
        "\n",
        "\n",
        "**DESCRIPTION:**\n",
        "\n",
        "This project aims to show the impact of an intervention (such as a Clean Air Zone) on NO₂ by building a Random Forest machine learning model. The model is trained on data prior to the introduction of the intervention- using meteorological variables such as wind speed, wind direction, and temperature, along with time-based features.\n",
        "\n",
        "The trained model is then used to predict NO₂ concentrations for after the intervention is introduced to extract NO₂ concentrations as if there had been no intervention introduced. These counterfactual predictions are compared with actual observed values during the intervention period to estimate the change in pollutant levels attributable to the intervention. COVID-19 periods, during which traffic and pollution levels dropped significantly, is disregarded for model training inline literature on the topic.\n",
        "\n",
        "This approach is taken for NO₂ analyses as NO₂ concentrations are greatly affected by meteorological variables. This makes NO₂ analysis hard as changes in concentrations are often hidden by meteorological factors. The machine learning model essentially takes out the influence of meteorological variables and makes impacts of interventions easy to analyse.\n",
        "\n",
        "This workbook is based off work from my University dissertation project \"Using machine learning to assess the impact of Newcastle and Sheffield’s Clean Air Zones on Nitrogen dioxide\". This workbook has been adapted from the dissertation so that any intervention on NO₂ can be investigated as long as there is enough DEFRA data available for the site\n",
        "\n",
        "**OBJECTIVES:**\n",
        "\n",
        "- Develop a regression-based time series model to predict NO₂ levels using weather and temporal features.\n",
        "- Generate counterfactual pollutant predictions for the post intervention period.\n",
        "- Quantify and visualise the estimated impact of the intervention by comparing predicted (no-intervention) vs actual (with-intervention) values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZqabxPrTSdv"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKSuOJ6LTR7b"
      },
      "outputs": [],
      "source": [
        "from astropy.time import Time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import seaborn as sns\n",
        "import scipy.stats as stats\n",
        "from scipy.stats import randint\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tabulate import tabulate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAr4eUSDSCVO"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yZXUeMUQs_D"
      },
      "outputs": [],
      "source": [
        "Site = '' # Your site name\n",
        "Extra_site_info = '' # Optional\n",
        "Start_date = pd.to_datetime('', dayfirst=True) # Intervention Start, dd-mm-yyyy\n",
        "Intervention_type = '' # Intervention Type\n",
        "COVID_start = pd.to_datetime('16-03-2020', dayfirst=True)\n",
        "COVID_end =  pd.to_datetime('01-03-2022', dayfirst=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coE-gazdy2A_"
      },
      "source": [
        "# **Load Data**\n",
        "- DEFRA data is available to download here(https://uk-air.defra.gov.uk/data/data_selector)\n",
        "- When downloading data from the DEFRA data there is a 5 year limit for downloaded data- section below loads the raw data from DEFRA and combines them into one df\n",
        "\n",
        "##**Steps to download data:**\n",
        "\n",
        "- DEFRA website (https://uk-air.defra.gov.uk/data/data_selector)\n",
        "- Select: Serach Hourly Networks\n",
        "\n",
        "Select Data Type- 'Measured Data'\n",
        "\n",
        "Select date Range- 'Custom Date (below)'- indvuial choice (it is best to try get as much data as possible to train the model- select dates where metelogical data is availe- aim for ast least 6 years of training data (data before the intervention) inline with literature. As mentioned above, data is only available in 5 year increments)   \n",
        "\n",
        "Select Monitoring Sites- 'Local Authority'- select the area of interest- then individual site\n",
        "\n",
        "Select Pollutants- 'Pollutant Name'- 'Select All'\n",
        "\n",
        "Select Output Type- 'Data to Email Address (CSV)' - enter email and Agree Terms\n",
        "\n",
        "'Get Data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuA84CJyGWTS"
      },
      "outputs": [],
      "source": [
        "# Replace \"YOURFILEPATH\" with the path to your DEFRA CSV\n",
        "# Adjust 'skiprows' depending on how many header lines in CSV\n",
        "\n",
        "# 2010-2015\n",
        "df_10to15 = pd.read_csv('YOURFILEPATH',skiprows= --, skipfooter=1,\n",
        "                        engine=\"python\")\n",
        "\n",
        "# 2015-2020\n",
        "df_15to20 = pd.read_csv('YOURFILEPATH', skiprows= --, skipfooter=1,\n",
        "                        engine=\"python\")\n",
        "\n",
        "# 2020-2025\n",
        "df_20to25 = pd.read_csv('YOURFILEPATH', skiprows= --, skipfooter=1,\n",
        "                        engine=\"python\")\n",
        "\n",
        "# combine into one\n",
        "df = pd.concat([df_10to15, df_15to20, df_20to25])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZmvC5wEDqOh"
      },
      "source": [
        "##Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fClWYNRsDB3s"
      },
      "outputs": [],
      "source": [
        "# change 'No data' to nan\n",
        "df.replace('No data', np.nan, inplace=True)\n",
        "\n",
        "# format date\n",
        "df['Date'] = pd.to_datetime(df['Date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvgbv5MRHcI7"
      },
      "outputs": [],
      "source": [
        "# Rename columns for simplicity\n",
        "df = df.rename(columns={'Modelled Temperature': 'Temp',\n",
        "                                          'Modelled Wind Speed': 'WindSpeed',\n",
        "                                          'Modelled Wind Direction': 'WindDir',\n",
        "                                          'Nitrogen dioxide':'NO2'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsGXa66YC32e"
      },
      "outputs": [],
      "source": [
        "# extract relevant columns\n",
        "df= df[\n",
        "    ['Date','Time','NO2', 'WindDir', 'WindSpeed', 'Temp']]\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-KY9ZKZlNog"
      },
      "source": [
        "# **Data Summary**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcvpfmFOIXpm"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPBuGvvDkoUT"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcBCLD-JlB42"
      },
      "outputs": [],
      "source": [
        "df.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKkambYYIKPD"
      },
      "source": [
        "# **Missing data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IU1I7DXLfKu"
      },
      "source": [
        "## Exploratory Analysis of Missing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iNHDnlblJfX"
      },
      "outputs": [],
      "source": [
        "# Missing data summary\n",
        "missing_data = df.isnull().sum()\n",
        "print(missing_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHLm1c92mr3O"
      },
      "outputs": [],
      "source": [
        "# Which months have most missing data?\n",
        "missing_data_months =(\n",
        "    df[df.isnull().any(axis=1)]['Date'].dt.to_period('M').value_counts())\n",
        "\n",
        "# Filter to have more than 10 missing\n",
        "missing_data_month = missing_data_months[missing_data_months > 10]\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(20, 6))\n",
        "plt.bar(missing_data_month.index.astype(str), missing_data_month.values)\n",
        "plt.xticks(rotation=90)\n",
        "plt.tight_layout()\n",
        "plt.title('Missing Data by Month')\n",
        "plt.xlabel('Month')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0AnQk41pUj6"
      },
      "source": [
        "A month of 30 days has ~720 data points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I23q5cuwq8Wl"
      },
      "outputs": [],
      "source": [
        "# Which months have most missing data in each key variable?\n",
        "\n",
        "variables = ['NO2', 'WindDir', 'WindSpeed', 'Temp']\n",
        "colours = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
        "\n",
        "fig, axes = plt.subplots(2,2, figsize=(18, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, var in enumerate(variables):\n",
        "\n",
        "    missing_data_months = (\n",
        "        df[df[var].isnull()]['Date'].dt.to_period('M').value_counts())\n",
        "    missing_data_months = (\n",
        "        missing_data_months[missing_data_months > 10].sort_index())\n",
        "    # Remove sort_index() to order by values\n",
        "\n",
        "    ax = axes[i]\n",
        "    x_labels = missing_data_months.index.astype(str)\n",
        "    ax.bar(x_labels, missing_data_months.values, color=colours[i])\n",
        "    ax.set_title(f\"Missing {var} by Month\")\n",
        "    ax.set_ylabel(\"Missing Count\")\n",
        "    ax.tick_params(axis='x', rotation=90, labelsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GciySUM6suOW"
      },
      "source": [
        "## Impute Missing Data\n",
        "\n",
        "- if available, missing meterolgocial data can instead be filled using local histricoal met data- such as HadUK-Grid dataset or MIDAS-Open - (https://www.metoffice.gov.uk/research/climate/maps-and-data/data/index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z03o6C0ULtZY"
      },
      "outputs": [],
      "source": [
        "# create a colloum for month and hour\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Hour'] = df['Time'].str[0:2].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYzqPEaHOSgv"
      },
      "outputs": [],
      "source": [
        "# NO2 and Temp- Impute nan using monthly/hourly median\n",
        "for col in ['NO2', 'Temp','WindSpeed']:\n",
        "  df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "  df[col] = (\n",
        "      df.groupby(['Month', 'Hour'])[col].transform(lambda x: x.fillna(x.median())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eewj7wxCN3fX"
      },
      "outputs": [],
      "source": [
        "# Wind\n",
        "\n",
        "df['WindDir'] = pd.to_numeric(df['WindDir'], errors='coerce')\n",
        "df['WindSpeed'] = pd.to_numeric(df['WindSpeed'], errors='coerce')\n",
        "\n",
        "radians = np.deg2rad(df['WindDir'])\n",
        "df['WindDir_sin'] = np.sin(radians)\n",
        "df['WindDir_cos'] = np.cos(radians)\n",
        "\n",
        "# Set Date as index for time based interpolation\n",
        "df = df.set_index('Date')\n",
        "\n",
        "df['WindSpeed'] = df['WindSpeed'].interpolate(method='time')\n",
        "df['WindDir_sin'] = df['WindDir_sin'].interpolate(method='time')\n",
        "df['WindDir_cos'] = df['WindDir_cos'].interpolate(method='time')\n",
        "\n",
        "# reset index\n",
        "df = df.reset_index()\n",
        "\n",
        "# Drop WindDir\n",
        "df = df.drop(columns=['WindDir'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ssn7i3cYtU6G"
      },
      "source": [
        "# Exploratory Data Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28mt_A1qtaeO"
      },
      "outputs": [],
      "source": [
        "variables = [ 'NO2', 'Temp', 'WindSpeed', 'WindDir_sin', 'WindDir_cos']\n",
        "colour = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
        "\n",
        "fig, axes = plt.subplots(2,3, figsize=(18, 8))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, var in enumerate(variables):\n",
        "\n",
        "    ax = axes[i]\n",
        "    x_labels = missing_data_months.index.astype(str)\n",
        "    ax.plot(df['Date'], df[var], label=var, color=colours[i], linewidth=0.8)\n",
        "\n",
        "    ax.set_title(f\"{var}\")\n",
        "    ax.tick_params(axis='x', rotation=90, labelsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5bZ-r799g1e"
      },
      "outputs": [],
      "source": [
        "# Pairplot\n",
        "sns.set(rc={'figure.figsize':(10,8)})\n",
        "sns.pairplot(df[[ 'NO2', 'Temp', 'WindSpeed', 'WindDir_sin', 'WindDir_cos']],\n",
        "             diag_kind='kde')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1-oTypY_Dnn"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(df[variables].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title(\"Correlation Between Variables\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Fgo4oZv-HLj"
      },
      "outputs": [],
      "source": [
        "monthly_avg = df.groupby('Month')[[ 'NO2', 'Temp']].mean()\n",
        "\n",
        "monthly_avg.plot(kind='bar', figsize=(12, 6))\n",
        "plt.title(\"Monthly Average: NO2, and Temperature\")\n",
        "plt.xlabel(\"Month\")\n",
        "plt.ylabel(\"Average Value\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFZtIesV-Vbp"
      },
      "outputs": [],
      "source": [
        "hourly_avg = df.groupby('Hour')[[ 'NO2', 'Temp']].mean()\n",
        "\n",
        "hourly_avg.plot(figsize=(12, 6))\n",
        "plt.title(\"Average Daily Cycle for NO2\")\n",
        "plt.xlabel(\"Hour of Day\")\n",
        "plt.ylabel(\"Average Concentration\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rINgKVBFTg-U"
      },
      "source": [
        "# Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jScrPiasTeWh"
      },
      "source": [
        "## Extreme vaules/outliares\n",
        "Note:\n",
        "The Air Quality Standards Regulations (2010) require that the annual mean concentration of NO₂ must not exceed 40 µg/m3 and that there should be no more than 18 exceedances of the hourly mean limit value (concentrations above 200 µg/m3) in a single year.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOHEJyDfXb22"
      },
      "source": [
        "High vaules >200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZIuyIDOXbG7"
      },
      "outputs": [],
      "source": [
        "Over200 = df[(df['NO2'] >=200)]\n",
        "print(Over200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2-Wm6RMWS0-"
      },
      "source": [
        "Identify NO₂ Spikes\n",
        "- above 40 and 1.5* the vaule before and after"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5jaoAM-WWdG"
      },
      "outputs": [],
      "source": [
        "spike = df[\n",
        "    (df['NO2'] > 40) &\n",
        "    (df['NO2'] >= 1.5 * df['NO2'].shift(1)) &\n",
        "    (df['NO2'] >= 1.5* df['NO2'].shift(-1))\n",
        "]\n",
        "print(spike)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKvm658JUyDX"
      },
      "source": [
        "Check for impossible negative numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGWcRTBSTitR"
      },
      "outputs": [],
      "source": [
        "# Check for negatives in key columns\n",
        "for col in ['NO2', 'WindSpeed']:\n",
        "    neg_rows = df[df[col] < 0]\n",
        "    if not neg_rows.empty:\n",
        "        print(f\"Negative values found in {col}:\")\n",
        "        print(neg_rows, \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvUfeS6aVV0t"
      },
      "outputs": [],
      "source": [
        "# Replace negatives with NaN and impute by Month + Hour median\n",
        "for col in ['NO2', 'WindSpeed']:\n",
        "    df.loc[df[col] < 0, col] = np.nan\n",
        "    nan_count = df[col].isna().sum()\n",
        "    df[col] = df.groupby(['Month', 'Hour'])[col].transform(\n",
        "        lambda x: x.fillna(x.median()))\n",
        "    print(f\"{col}: {nan_count} values imputed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uHZAMIJYVqL"
      },
      "source": [
        "## Feature Engineering\n",
        "- Julian day\n",
        "- Day of week\n",
        "- is weekday?\n",
        "<!-- - is covid? -->\n",
        "- is spike?\n",
        "- is high vaule\n",
        "- day of year\n",
        "-  day of year sin- cos\n",
        "- hour sin cos\n",
        "- rush hour?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQbawFK4ZewP"
      },
      "outputs": [],
      "source": [
        "# Temporal\n",
        "df['DoW'] = df['Date'].dt.dayofweek # Day of week (Monday = 0, Sunday = 6)\n",
        "df['JD'] = df['Date'].apply(lambda x: Time(x).jd) # Julian day\n",
        "df['DayOfYear'] = df['Date'].dt.dayofyear # Day of year\n",
        "df['ISweekday']= df['DoW'].apply(lambda x: 1 if x in [0,1,2,3,4] else 0) #Is weekday?\n",
        "df['ISrushhour'] = df.apply(\n",
        "    lambda row: 1 if (row['ISweekday'] == 1 and (\n",
        "        7 <= row['Hour'] <= 9 or 16 <= row['Hour'] <= 18)) else 0, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMR9swdXLfWL"
      },
      "outputs": [],
      "source": [
        "# sin and cos\n",
        "df['DayOfYear_sin'] = np.sin(2 * np.pi * df['DayOfYear'] / 365)\n",
        "df['DayOfYear_cos'] = np.cos(2 * np.pi * df['DayOfYear'] / 365)\n",
        "df['Hour_sin'] = np.sin(2 * np.pi * df['Hour'] / 24)\n",
        "df['Hour_cos'] = np.cos(2 * np.pi * df['Hour'] / 24)\n",
        "\n",
        "  # wind direction sin & cos already calculated during Impute Missing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRp2KVwTelmt"
      },
      "outputs": [],
      "source": [
        "# NO2 vaules\n",
        "df['spike'] = ((df['NO2'] > 40) &\n",
        "               (df['NO2'] >= 1.5 * df['NO2'].shift(1)) &\n",
        "               (df['NO2'] >= 1.5 * df['NO2'].shift(-1))).astype(int) # is spike?\n",
        "df['isHigh'] = (df['NO2'] >= 200).astype(int) # is NO2 over(or=) 200?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_77ezOqf_U8"
      },
      "outputs": [],
      "source": [
        "print(df.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UlxsSY5gelq"
      },
      "source": [
        "## Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "to32anI0yFeB"
      },
      "outputs": [],
      "source": [
        "# scale\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(df[['Temp', 'WindSpeed']])\n",
        "\n",
        "df[['Temp', 'WindSpeed']] = scaler.transform(\n",
        "    df[['Temp', 'WindSpeed',]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nu2KVXUmz-8u"
      },
      "source": [
        "##Catgorise numbers (numerical or categorical)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcO-AelOz_19"
      },
      "outputs": [],
      "source": [
        "# Numerical\n",
        "df[['NO2','JD', 'Temp', 'WindSpeed',  'WindDir_sin', 'WindDir_cos']] = (\n",
        "    df[['NO2','JD', 'Temp', 'WindSpeed', 'WindDir_sin', 'WindDir_cos']]\n",
        "    .apply(pd.to_numeric, errors='coerce')\n",
        ")\n",
        "# categorical\n",
        "df [['Month', 'Hour', 'DoW', 'DayOfYear']] = (\n",
        "    df [['Month', 'Hour', 'DoW', 'DayOfYear']].astype('category')\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WopdWNW90Y9b"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih10vjGo7J4f"
      },
      "source": [
        "## Correlation plots with additional variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1wmExGE7W3t"
      },
      "outputs": [],
      "source": [
        "# Pairplot\n",
        "sns.set(rc={'figure.figsize':(10,8)})\n",
        "sns.pairplot(df[[ 'NO2', 'WindSpeed', 'Temp', 'Month', 'Hour',\n",
        "       'WindDir_sin', 'WindDir_cos', 'DoW', 'JD', 'DayOfYear','ISweekday',\n",
        "                  'ISrushhour', 'DayOfYear_sin', 'DayOfYear_cos', 'Hour_sin',\n",
        "       'Hour_cos']], diag_kind='kde')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GdtMQG37fJP"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(40, 30))\n",
        "sns.heatmap(df[['NO2', 'WindSpeed', 'Temp', 'Month', 'Hour','WindDir_sin',\n",
        "                'WindDir_cos', 'DoW', 'JD', 'DayOfYear', 'ISweekday',\n",
        "                'ISrushhour', 'DayOfYear_sin', 'DayOfYear_cos', 'Hour_sin',\n",
        "       'Hour_cos']].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title(\"Correlation Between Variables\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9CXzpNwiG_j"
      },
      "source": [
        "# Extract training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GuqSKUCiLWu"
      },
      "outputs": [],
      "source": [
        "Training = df[\n",
        "    ((df['Date'] < COVID_start) | (df['Date'] >COVID_end)) &\n",
        "     (df['Date'] < Start_date)] #remove covid period"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hEEGfDciTsW"
      },
      "source": [
        "#Extract Feature/ Target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsQ1rdTnicdB"
      },
      "outputs": [],
      "source": [
        "feature_train = Training[['WindSpeed', 'Temp', 'Month', 'Hour','WindDir_sin',\n",
        "                          'WindDir_cos', 'DoW', 'JD', 'DayOfYear','ISweekday',\n",
        "                          'ISrushhour', 'DayOfYear_sin', 'DayOfYear_cos',\n",
        "                          'Hour_sin','Hour_cos', 'spike','isHigh']]\n",
        "\n",
        "target_train = Training['NO2']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSmz67VgjLSp"
      },
      "source": [
        "#The random forest\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmPMX5uw8-AH"
      },
      "outputs": [],
      "source": [
        "rf = RandomForestRegressor(random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSj6Sdw-i8dL"
      },
      "source": [
        "## Split train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlIbpI6ZjDwT"
      },
      "outputs": [],
      "source": [
        "feature_train, feature_validation, target_train, target_validation =(\n",
        "    train_test_split(feature_train, target_train, test_size=0.2,\n",
        "                     random_state=42)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bz7_eF5-8z8Q"
      },
      "source": [
        "## Optimise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gpZl3Bs837B"
      },
      "source": [
        "Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OY5X72FSz3oz"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter distributions\n",
        "param_dist = {\n",
        "    'n_estimators': randint(100, 1000),\n",
        "    'max_depth': [None] + list(range(5, 30, 5)),\n",
        "    'min_samples_split': randint(2, 20),\n",
        "    'min_samples_leaf': randint(1, 10),\n",
        "    'max_features': ['sqrt', 'log2']\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bchfVFAB1m9n"
      },
      "source": [
        "RandomizedSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDrwxPsx1nUb"
      },
      "outputs": [],
      "source": [
        "random_search = RandomizedSearchCV(\n",
        "    rf, param_distributions=param_dist, n_iter=50, cv=5, scoring='r2',\n",
        "    random_state=42, n_jobs=-1, verbose=2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36oaIQGl9IXT"
      },
      "source": [
        "Fit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CNogTtto7aU0"
      },
      "outputs": [],
      "source": [
        "random_search.fit(feature_train, target_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlRQTtv-9OqG"
      },
      "source": [
        "Optimised Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "t4mjllkM7pe3"
      },
      "outputs": [],
      "source": [
        "best_params = random_search.best_params_\n",
        "\n",
        "# Print Best parameters and R²\n",
        "print(\"Best parameters:\", best_params)\n",
        "print(\"Best CV R²:\", random_search.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gz-3VM5hDwZt"
      },
      "source": [
        "## Run Data with Optimised Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KI_L5eyyDux6"
      },
      "outputs": [],
      "source": [
        "rf = RandomForestRegressor(\n",
        "    n_estimators=best_params['n_estimators'],\n",
        "    max_depth=best_params['max_depth'],\n",
        "    max_features=best_params['max_features'],\n",
        "    min_samples_split=best_params['min_samples_split'],\n",
        "    min_samples_leaf=best_params['min_samples_leaf'],\n",
        "    random_state=42\n",
        ").fit(feature_train, target_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFBudBzQ7YjE"
      },
      "source": [
        "# Evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcp2-w9M7Vz6"
      },
      "outputs": [],
      "source": [
        "target_pred = rf.predict(feature_validation)\n",
        "target_pred_train = rf.predict(feature_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeE3vzUK7d_Q"
      },
      "outputs": [],
      "source": [
        "  # generalisation\n",
        "mse_g = mean_squared_error(target_validation, target_pred) # Mean Squared Error\n",
        "r2_g = r2_score(target_validation, target_pred) # R-squared\n",
        "\n",
        "  # Self-fit\n",
        "mse_sf = mean_squared_error(target_train, target_pred_train) #Mean Squared Error\n",
        "r2_sf = r2_score(target_train, target_pred_train) # R-squared"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xw_u8piZ76zF"
      },
      "outputs": [],
      "source": [
        "# Table of model fit\n",
        "modelfit_table = [\n",
        "    [\"Root Mean Squared Error\", f'{np.sqrt(mse_g):.2f}',\n",
        "     f'{np.sqrt(mse_sf):.2f}'],\n",
        "    [\"R-squared\", f'{r2_g:.2f}', f'{r2_sf:.2f}'],\n",
        "]\n",
        "\n",
        "# Display\n",
        "header_mf = [f'Model fitemnt at {Site}', \"Generalisation\", \"Self Fit\"]\n",
        "print(tabulate(modelfit_table, headers= header_mf, tablefmt=\"grid\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rD2pYlL07jmT"
      },
      "source": [
        "## Run on dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADRiRTRf7pSW"
      },
      "outputs": [],
      "source": [
        "df_feature = df[['WindSpeed', 'Temp', 'Month', 'Hour','WindDir_sin',\n",
        "                          'WindDir_cos', 'DoW', 'JD', 'DayOfYear','ISweekday',\n",
        "                          'ISrushhour', 'DayOfYear_sin', 'DayOfYear_cos',\n",
        "                          'Hour_sin','Hour_cos', 'spike','isHigh']]\n",
        "ALL_predict = rf.predict(df_feature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ck0seETG7_B-"
      },
      "outputs": [],
      "source": [
        "# Calculate Residuals\n",
        "Raw_residuals = df['NO2'] - ALL_predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vzsiHUtSnjC"
      },
      "source": [
        "# **Results**\n",
        "- Line Plot\n",
        "- Summary Table (%change and P vaule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ew138cKQ8E4P"
      },
      "source": [
        "## Line plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIDekcRKk6ab"
      },
      "outputs": [],
      "source": [
        "# Smooth\n",
        "true_365 = (\n",
        "    df['NO2'].rolling(window=(24*365), center=True, min_periods=1).median())\n",
        "predicted_365 = (\n",
        "    pd.Series(ALL_predict).rolling(window=(24*365), center=True,\n",
        "                                   min_periods=1).median())\n",
        "residuals_365 = (\n",
        "    Raw_residuals.rolling(window=(24*365), center=True, min_periods=1).median())\n",
        "  # 7 day\n",
        "residuals_7 = (\n",
        "    Raw_residuals.rolling(window=(24*7), center=True, min_periods=1).median())\n",
        "\n",
        "# Plotting\n",
        "RF_plot, ax1 = plt.subplots(figsize=(14, 8))\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "  # Actual and Predicted\n",
        "true_365_line, = ax1.plot(df['Date'], true_365, color='blue',\n",
        "                          linestyle='-', label='Actual annual NO₂')\n",
        "predicted_365_line, = ax1.plot(df['Date'], predicted_365, color='orange',\n",
        "                               linestyle='-', label='Predicted annual NO₂')\n",
        "\n",
        "  # Residuals\n",
        "residuals_365_line, = (\n",
        "    ax2.plot(df['Date'], residuals_365, color='green', linestyle= (0,(5,5)),\n",
        "             label='Residual annual NO₂'))\n",
        "residual_daily_line, = (\n",
        "    ax2.plot(df['Date'], residuals_7, color='green', alpha=0.3, linestyle='-',\n",
        "             label='Raw Residual NO₂'))\n",
        "\n",
        "  # x and y lim\n",
        "ax1.set_xlim(df['Date'].min(), df['Date'].max())\n",
        "# ax2.set_ylim (-15, 7)\n",
        "\n",
        "  # Format x-axis to show ticks only at the start of each year\n",
        "ax1.xaxis.set_major_locator(mdates.YearLocator())\n",
        "ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
        "RF_plot.autofmt_xdate()\n",
        "\n",
        "  # Perfect Prediction\n",
        "ax2.axhline(y=0, color='black', linestyle=':', linewidth=2,\n",
        "            label='Perfect Prediction') # 0 rediuals\n",
        "x_max = ax2.get_xlim()[1]\n",
        "ax2.text(x_max * 0.999, 0, 'Perfect Prediction', verticalalignment='bottom',\n",
        "         horizontalalignment='right', fontsize=9, color='black', weight='bold')\n",
        "\n",
        "#  Highlight COVID and Intervention\n",
        "    # COVID\n",
        "ax1.axvspan(COVID_start, COVID_end, color='gray', alpha=0.3)\n",
        "x_covid_label = COVID_start + (COVID_end - COVID_start) / 2\n",
        "y_covid_label = ax1.get_ylim()[0] * 1.1\n",
        "ax1.text(x_covid_label, y_covid_label, 'COVID', ha='center', va='center',\n",
        "         color='black', fontsize=10, weight='bold')\n",
        "ax1.axvline(COVID_start, color='black', linestyle='-')\n",
        "ax1.axvline(COVID_end, color='black', linestyle='-')\n",
        "\n",
        "    # Intervention\n",
        "ax1.axvspan(Start_date, df['Date'].max(), color='blue', alpha=0.1)\n",
        "x_intervention_label = Start_date + (df['Date'].max() - Start_date) / 2\n",
        "y_intervention_label = ax1.get_ylim()[0] * 1.1\n",
        "ax1.text(x_intervention_label, y_intervention_label, f'{Intervention_type}',\n",
        "         ha='center',va='center', color='black', fontsize=10, weight='bold')\n",
        "ax1.axvline(Start_date, color='black', linestyle='-')\n",
        "\n",
        "# Combine legends from both axes\n",
        "lines =[true_365_line, predicted_365_line, residuals_365_line ,\n",
        "        residual_daily_line]\n",
        "labels = [line.get_label() for line in lines]\n",
        "  # Format legend\n",
        "legend = ax1.legend(lines, labels, loc='lower left', frameon=True)\n",
        "frame = legend.get_frame()\n",
        "frame.set_facecolor(\"white\")\n",
        "frame.set_alpha(1)\n",
        "frame.set_edgecolor(\"black\")\n",
        "frame.set_linewidth(.5)\n",
        "\n",
        "#  R²\n",
        "ax1.text(\n",
        "    0.03, 0.24, f'R²: {r2_g:.2f}',transform=ax1.transAxes, ha='left', va='top',\n",
        "    fontsize=12, weight='bold')\n",
        "\n",
        "# Labels and title\n",
        "ax1.set_xlabel('Date', weight='bold')\n",
        "ax1.set_ylabel('NO₂ (µg/m³)', weight='bold')\n",
        "ax1.set_title(\n",
        "    f'Residual, Actual and Predicted NO₂ at {Site} {Extra_site_info}',\n",
        "    weight='bold')\n",
        "ax1.grid(True, color='black',alpha=0.2)\n",
        "ax2.grid(False)\n",
        "ax2.set_ylabel('Residual NO₂ (µg/m³)(Actual - Predicted)', color='green',\n",
        "               weight='bold')\n",
        "ax2.tick_params(axis='y', labelcolor='green')\n",
        "\n",
        "# Format plot\n",
        "  # Plot Background\n",
        "ax1.set_facecolor(\"white\")\n",
        "ax2.set_facecolor(\"white\")\n",
        "    # Plot Boarder\n",
        "for spine in ax1.spines.values():\n",
        "    spine.set_edgecolor(\"black\")\n",
        "    spine.set_linewidth(.75)\n",
        "for spine in ax2.spines.values():\n",
        "    spine.set_edgecolor(\"black\")\n",
        "    spine.set_linewidth(.75)\n",
        "\n",
        "RF_plot.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6_KOIdS8Jn9"
      },
      "source": [
        "## Table- post intervation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jNdICi28KcF"
      },
      "outputs": [],
      "source": [
        "# Crop\n",
        "Post_true = df[df['Date'] >= Start_date]\n",
        "Post_predict = ALL_predict[df['Date'] >= Start_date]\n",
        "\n",
        "# Smooth over 1 year (24 hours * 365 days)\n",
        "true_365 = (\n",
        "    Post_true['NO2'].rolling(window=(24*365),\n",
        "                             center=False, min_periods=1).median())\n",
        "predicted_365 =(\n",
        "    pd.Series(Post_predict).rolling(window=(24*365),\n",
        "                                    center=False, min_periods=1).median())\n",
        "\n",
        "# Mean\n",
        "true_mean = true_365.mean()\n",
        "predicted_mean = predicted_365.mean()\n",
        "\n",
        "# Percentage change\n",
        "pc = (true_mean - predicted_mean) / predicted_mean * 100\n",
        "\n",
        "# P-value\n",
        "p_value = stats.ttest_ind(true_365.dropna(),\n",
        "                          predicted_365.dropna(), equal_var=False).pvalue\n",
        "\n",
        "# Table\n",
        "results_table = [\n",
        "    [f'Predicted Mean (NO₂ if the {Intervention_type} was not in place)',\n",
        "     round(predicted_mean, 2)],\n",
        "    [\"True Mean\", round(true_mean, 2)],\n",
        "    [\"% Change\", round(pc, 2)],\n",
        "    [\"P-value\", round(p_value, 4)],\n",
        "]\n",
        "\n",
        "# Display\n",
        "header = [\" \", f'NO₂ at {Site} after {Intervention_type}']\n",
        "print(tabulate(results_table, headers=header, tablefmt=\"grid\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "gZqabxPrTSdv",
        "SAr4eUSDSCVO",
        "coE-gazdy2A_",
        "D-KY9ZKZlNog",
        "AKkambYYIKPD",
        "GciySUM6suOW",
        "Ssn7i3cYtU6G",
        "rINgKVBFTg-U",
        "5UlxsSY5gelq",
        "y9CXzpNwiG_j",
        "9hEEGfDciTsW",
        "zFBudBzQ7YjE"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}